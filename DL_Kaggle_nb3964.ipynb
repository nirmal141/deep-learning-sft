{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LL7bsp9a6Buf"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y unsloth trl transformers peft accelerate bitsandbytes tokenizers\n",
        "\n",
        "!pip install -U \"numpy>=2.0.0\"\n",
        "\n",
        "!pip install -U \\\n",
        "  \"transformers==4.46.2\" \\\n",
        "  \"trl==0.23.1\" \\\n",
        "  \"peft==0.13.2\" \\\n",
        "  \"accelerate==0.34.2\" \\\n",
        "  \"bitsandbytes==0.44.1\"\n",
        "\n",
        "!pip install -U \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpPfEFdS6XJe"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "print(\"All imports successful!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_jvc-yV4uSZ"
      },
      "outputs": [],
      "source": [
        "#configurations\n",
        "MAX_SEQ_LENGTH = 2048\n",
        "LOAD_IN_4BIT = True\n",
        "\n",
        "LORA_R = 16\n",
        "LORA_ALPHA = 32\n",
        "LORA_DROPOUT = 0.05\n",
        "\n",
        "TRAIN_SAMPLES = 90000 #our latest model score was trained on 90000 samples\n",
        "\n",
        "PER_DEVICE_BATCH_SIZE = 4\n",
        "GRADIENT_ACCUMULATION_STEPS = 2\n",
        "\n",
        "LEARNING_RATE = 2e-4\n",
        "NUM_EPOCHS = 1 #number of epochs we trained for\n",
        "WARMUP_RATIO = 0.03\n",
        "WEIGHT_DECAY = 0.01\n",
        "MAX_GRAD_NORM = 1.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBolFBxz6Vm2"
      },
      "outputs": [],
      "source": [
        "#loading the model \n",
        "DTYPE = torch.bfloat16\n",
        "print(\"Loading model...\")\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Meta-Llama-3.1-8B\",\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    dtype=DTYPE,\n",
        "    load_in_4bit=LOAD_IN_4BIT,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHd8LYFr6XWW"
      },
      "outputs": [],
      "source": [
        "#prompting and cleaning the text\n",
        "TRAINING_PROMPT = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "You are a mathematical verification expert. Your task is to determine if a given answer to a math question is correct.\n",
        "\n",
        "Instructions:\n",
        "- Carefully analyze the question, the provided solution/reasoning, and the answer\n",
        "- Consider mathematical accuracy, logical reasoning, and computational correctness\n",
        "- Respond with ONLY \"True\" if the answer is correct, or \"False\" if incorrect\n",
        "- Do not provide explanations, just the boolean response<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Question: {}\n",
        "\n",
        "Solution/Reasoning: {}\n",
        "\n",
        "Provided Answer: {}\n",
        "\n",
        "Is the answer correct?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "{}\"\"\"\n",
        "\n",
        "def clean_text(text):\n",
        "    #cleaning and normalizing text\n",
        "    if text is None:\n",
        "        return \"\"\n",
        "    text = re.sub(r'\\s+', ' ', str(text))\n",
        "    text = text.replace('```python', '').replace('```', '')\n",
        "    return text.strip()\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    #format dataset into training prompts\n",
        "    questions = examples[\"question\"]\n",
        "    solutions = examples[\"solution\"]\n",
        "    answers = examples[\"answer\"]\n",
        "    outputs = examples[\"is_correct\"]\n",
        "\n",
        "    texts = []\n",
        "    for question, solution, answer, output in zip(questions, solutions, answers, outputs):\n",
        "        question = clean_text(question)\n",
        "        solution = clean_text(solution)\n",
        "        answer = clean_text(answer)\n",
        "        output_str = \"True\" if output else \"False\"\n",
        "\n",
        "        text = TRAINING_PROMPT.format(\n",
        "            question,\n",
        "            solution,\n",
        "            answer,\n",
        "            output_str\n",
        "        ) + tokenizer.eos_token\n",
        "\n",
        "        texts.append(text)\n",
        "\n",
        "    return {\"text\": texts}\n",
        "\n",
        "print(\"Loading dataset...\")\n",
        "full_dataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\", split=\"train\")\n",
        "\n",
        "print(f\"Full dataset size: {len(full_dataset)}\")\n",
        "shuffled = full_dataset.shuffle(seed=42)\n",
        "\n",
        "train_dataset = shuffled.select(range(TRAIN_SAMPLES))\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "\n",
        "formatted_train = train_dataset.map(formatting_prompts_func, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1RFdVDT6wQo"
      },
      "outputs": [],
      "source": [
        "#LoRA configuration\n",
        "print(\"Configuring LoRA...\")\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=LORA_R,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=0.05, #dropout rate\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=42,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeaOkG6F65VG"
      },
      "outputs": [],
      "source": [
        "#setting up the trainer\n",
        "print(\"Setting up trainer...\")\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=formatted_train,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    args=TrainingArguments(\n",
        "        output_dir=\"outputs\",\n",
        "        num_train_epochs=NUM_EPOCHS,\n",
        "        per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
        "        per_device_eval_batch_size=PER_DEVICE_BATCH_SIZE,\n",
        "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        warmup_ratio=WARMUP_RATIO,\n",
        "        weight_decay=WEIGHT_DECAY,\n",
        "        max_grad_norm=MAX_GRAD_NORM,\n",
        "        bf16=True, fp16=False,\n",
        "        optim=\"adamw_bnb_8bit\",\n",
        "        gradient_checkpointing=True,\n",
        "\n",
        "        eval_strategy=\"no\",\n",
        "        load_best_model_at_end=False,\n",
        "\n",
        "        logging_steps=100,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=500,\n",
        "        save_total_limit=3,\n",
        "        report_to=\"none\",\n",
        "        seed=42,\n",
        "        save_safetensors=True,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQ46ke4h697W"
      },
      "outputs": [],
      "source": [
        "print(\"Starting training...\")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVlJ-TTJ7cIP"
      },
      "outputs": [],
      "source": [
        "def parse_output(response_text):\n",
        "    #enhanced output parsing\n",
        "    if '<|start_header_id|>assistant<|end_header_id|>' in response_text:\n",
        "        assistant_response = response_text.split('<|start_header_id|>assistant<|end_header_id|>')[-1]\n",
        "        assistant_response = assistant_response.split('<|eot_id|>')[0].strip()\n",
        "\n",
        "        first_word = assistant_response.split()[0].lower() if assistant_response.split() else \"\"\n",
        "        if 'true' in first_word:\n",
        "            return True\n",
        "        if 'false' in first_word:\n",
        "            return False\n",
        "\n",
        "    response_lower = response_text.lower()\n",
        "    true_count = response_lower.count('true')\n",
        "    false_count = response_lower.count('false')\n",
        "\n",
        "    if true_count > false_count:\n",
        "        return True\n",
        "    elif false_count > true_count:\n",
        "        return False\n",
        "\n",
        "    last_part = response_text[-100:].lower()\n",
        "    if 'true' in last_part:\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "inference_prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "You are a mathematical verification expert. Your task is to determine if a given answer to a math question is correct.\n",
        "\n",
        "Instructions:\n",
        "- Carefully analyze the question, the provided solution/reasoning, and the answer\n",
        "- Consider mathematical accuracy, logical reasoning, and computational correctness\n",
        "- Respond with ONLY \"True\" if the answer is correct, or \"False\" if incorrect\n",
        "- Do not provide explanations, just the boolean response<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Question: {}\n",
        "\n",
        "Solution/Reasoning: {}\n",
        "\n",
        "Provided Answer: {}\n",
        "\n",
        "Is the answer correct?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nLoading test dataset...\")\n",
        "test_dataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\", split=\"test\")\n",
        "\n",
        "predictions = []\n",
        "print(\"\\nGenerating test predictions...\")\n",
        "\n",
        "for example in tqdm(test_dataset):\n",
        "    question = clean_text(example[\"question\"])\n",
        "    solution = clean_text(example[\"solution\"])\n",
        "    answer = clean_text(example.get(\"answer\", \"\"))\n",
        "\n",
        "    prompt = inference_prompt.format(question, solution, answer)\n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # Generate with low temperature\n",
        "    outputs = model.generate(**inputs, max_new_tokens=10, use_cache=True, temperature=0.1)\n",
        "    response = tokenizer.batch_decode(outputs)[0]\n",
        "\n",
        "    prediction = parse_output(response)\n",
        "    predictions.append(prediction)\n",
        "\n",
        "#creating the submission file\n",
        "submission = pd.DataFrame({\n",
        "    'ID': range(len(predictions)),\n",
        "    'is_correct': predictions\n",
        "})\n",
        "\n",
        "submission.to_csv('submission2.csv', index=False)\n",
        "print(f\"\\n Submission file created: submission.csv\")\n",
        "print(f\"  Total predictions: {len(predictions)}\")\n",
        "print(f\"  True predictions: {sum(predictions)}\")\n",
        "print(f\"  False predictions: {len(predictions) - sum(predictions)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOl9UIiyFUtZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
